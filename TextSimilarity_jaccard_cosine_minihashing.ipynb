{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Analysis\n",
    "\n",
    "we will focus on using Jaccard Similarity and Cosine Similarity to measure the similarity between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![./images/text_similarity.png](./images/text_similarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./data/sample1.txt','./data/sample2.txt','./data/sample3.txt','./data/sample4.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example here, we are assuming space as an unique character, \n",
    "Also capital letters are different than lower cased letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"sky is Blue\" = {['sk'], ['ky'], ['y '], ['is'], [' B'], ['Bl'], ['lu'],['ue']}\n",
    "def twoCharGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        textFile = data.read().replace('\\n', ' ')\n",
    "        kGrams = set()\n",
    "        # 2-Char gram\n",
    "        for i in range(len(textFile)-1):\n",
    "            if textFile[i] + textFile[i+1] not in kGrams:\n",
    "                kGrams.add(textFile[i] + textFile[i+1])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"sky is Blue\" = {['sky'], [' is'], [' Bl'], ['ue']}\n",
    "def threeCharGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        textFile = data.read().replace('\\n', ' ')\n",
    "        kGrams = set()\n",
    "        # 3-Char gram\n",
    "        for i in range(len(textFile)-2):\n",
    "            if textFile[i] + textFile[i+1] + textFile[i+2] not in kGrams:\n",
    "                kGrams.add(textFile[i] + textFile[i+1] + textFile[i+2])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"sky is Blue\" = {['sky is'], [' Blue']}\n",
    "def twoWordGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        tokens = str.split(data.read().replace('\\n', ' '))\n",
    "        kGrams = set()\n",
    "        #2-word gram\n",
    "        for i in range(len(tokens)-1):\n",
    "            if tokens[i] + ' ' + tokens[i+1] not in kGrams:\n",
    "                kGrams.add(tokens[i] + ' ' + tokens[i+1])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample1.txt two char gram size: 263\n",
      "./data/sample2.txt two char gram size: 262\n",
      "./data/sample3.txt two char gram size: 269\n",
      "./data/sample4.txt two char gram size: 255\n"
     ]
    }
   ],
   "source": [
    "for dataset in files:\n",
    "    print(dataset + ' two char gram size: %d' % len(twoCharGram(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample1.txt three char gram size: 765\n",
      "./data/sample2.txt three char gram size: 762\n",
      "./data/sample3.txt three char gram size: 828\n",
      "./data/sample4.txt three char gram size: 698\n"
     ]
    }
   ],
   "source": [
    "for dataset in files:\n",
    "    print(dataset + ' three char gram size: %d' % len(threeCharGram(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample1.txt two word gram size: 279\n",
      "./data/sample2.txt two word gram size: 278\n",
      "./data/sample3.txt two word gram size: 337\n",
      "./data/sample4.txt two word gram size: 232\n"
     ]
    }
   ],
   "source": [
    "for dataset in files:\n",
    "    print(dataset + ' two word gram size: %d' % len(twoWordGram(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the similarity between documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity is defined\n",
    "$JS(A,B)=$\n",
    "> # $\\frac{|A\\cap B|}{|A\\cap B|+ |A\\Delta B|} = \\frac{|A\\cap B|}{|A\\cup B|}$\n",
    "\n",
    "Consider two sets A = {0,1,2,5,6} and B = {0,2,3,5,7,9}. How similar are A and B? The Jaccard similarity is defined\n",
    "\n",
    "$JS(A, B) = \\frac{|A \\cap B|}{|A\\cup B|}$\n",
    "$= \\frac{|{0,2,5}|}{|{0,1,2,3,5,6,7,9}|} =  \\frac{3}{8} = 0.375$\n",
    "\n",
    "Given a set A, the cardinality of A denoted |A| counts how many elements are in A. The intersection between two sets A and B is denoted A ∩ B and reveals all items which are in both sets. The union between two sets A and B is denoted A ∪ B and reveals all items which are in either set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 1: AI is our friend and it has been friendly <br>\n",
    "Sentence 2: AI and humans have always been friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Jaccard Similarity](./images/sam.png)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1,s2,s3,s4, message):\n",
    "    print('sample1.txt\\'s '+ message +' similarity with sample2.txt is: %.6f percent' % (100.* len(s1.intersection(s2))/ len(s1.union(s2))))\n",
    "    print('sample1.txt\\'s '+ message +' similarity with sample3.txt is: %.6f percent' % (100.* len(s1.intersection(s3))/ len(s1.union(s3))))\n",
    "    print('sample1.txt\\'s '+ message +' similarity with sample4.txt is: %.6f percent' % (100.* len(s1.intersection(s4))/ len(s1.union(s4))))\n",
    "    print('sample2.txt\\'s '+ message +' similarity with sample3.txt is: %.6f percent' % (100.* len(s2.intersection(s3))/ len(s2.union(s3))))\n",
    "    print('sample2.txt\\'s '+ message +' similarity with sample4.txt is: %.6f percent' % (100.* len(s2.intersection(s4))/ len(s2.union(s4))))\n",
    "    print('sample3.txt\\'s '+ message +' similarity with sample4.txt is: %.6f percent' % (100.* len(s3.intersection(s4))/ len(s3.union(s4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt's two char gram similarity with sample2.txt is: 98.113208 percent\n",
      "sample1.txt's two char gram similarity with sample3.txt is: 81.569966 percent\n",
      "sample1.txt's two char gram similarity with sample4.txt is: 64.444444 percent\n",
      "sample2.txt's two char gram similarity with sample3.txt is: 80.000000 percent\n",
      "sample2.txt's two char gram similarity with sample4.txt is: 64.126984 percent\n",
      "sample3.txt's two char gram similarity with sample4.txt is: 65.299685 percent\n"
     ]
    }
   ],
   "source": [
    "s1set = twoCharGram(files[0])\n",
    "s2set = twoCharGram(files[1])\n",
    "s3set = twoCharGram(files[2])\n",
    "s4set = twoCharGram(files[3])\n",
    "jaccard(s1set,s2set,s3set,s4set, 'two char gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt's three char gram similarity with sample2.txt is: 97.797927 percent\n",
      "sample1.txt's three char gram similarity with sample3.txt is: 58.035714 percent\n",
      "sample1.txt's three char gram similarity with sample4.txt is: 30.508475 percent\n",
      "sample2.txt's three char gram similarity with sample3.txt is: 56.804734 percent\n",
      "sample2.txt's three char gram similarity with sample4.txt is: 30.590340 percent\n",
      "sample3.txt's three char gram similarity with sample4.txt is: 31.212382 percent\n"
     ]
    }
   ],
   "source": [
    "s1set = threeCharGram(files[0])\n",
    "s2set = threeCharGram(files[1])\n",
    "s3set = threeCharGram(files[2])\n",
    "s4set = threeCharGram(files[3])\n",
    "jaccard(s1set,s2set,s3set,s4set, 'three char gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt's two Word Gram similarity with sample2.txt is: 94.076655 percent\n",
      "sample1.txt's two Word Gram similarity with sample3.txt is: 18.234165 percent\n",
      "sample1.txt's two Word Gram similarity with sample4.txt is: 3.024194 percent\n",
      "sample2.txt's two Word Gram similarity with sample3.txt is: 17.366412 percent\n",
      "sample2.txt's two Word Gram similarity with sample4.txt is: 3.030303 percent\n",
      "sample3.txt's two Word Gram similarity with sample4.txt is: 1.607143 percent\n"
     ]
    }
   ],
   "source": [
    "s1set = twoWordGram(files[0])\n",
    "s2set = twoWordGram(files[1])\n",
    "s3set = twoWordGram(files[2])\n",
    "s4set = twoWordGram(files[3])\n",
    "jaccard(s1set,s2set,s3set,s4set, 'two Word Gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minhashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinHash uses the magic of hashing to quickly estimate Jaccard Similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to http://www.cs.utah.edu/~jeffp/DMBook/L4-Minhash for more information on Minhashing\n",
    "s1gram = threeCharGram(files[0])\n",
    "s2gram = threeCharGram(files[1])\n",
    "stotal = list(s1gram.union(s2gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with t = 20  we get a minhash similarity of  1.0\n",
      "with t = 60  we get a minhash similarity of  0.9833333333333333\n",
      "with t = 150  we get a minhash similarity of  0.98\n",
      "with t = 300  we get a minhash similarity of  0.99\n",
      "with t = 600  we get a minhash similarity of  0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "# Implementing fast Minhashing algortihm\n",
    "for k in [20,60,150,300,600]:    \n",
    "    successCounter = 0\n",
    "    for t in range (k):\n",
    "        minNum = [math.inf, math.inf]\n",
    "        for i in range (len(stotal)):\n",
    "            current = hash(str(t)+stotal[i]+str(t)) % 10000\n",
    "            if stotal[i] in s1gram: # this is how we'll emulate the vector representation of this sample 1\n",
    "                if (current < minNum[0]):\n",
    "                    minNum[0] = current\n",
    "            if stotal[i] in s2gram: # this is how we'll emulate the vector representation of this sample 2\n",
    "                if (current < minNum[1]):\n",
    "                    minNum[1] = current\n",
    "        if minNum[0] == minNum[1]:\n",
    "            successCounter = successCounter+1\n",
    "    print(\"with t = %d\"%k, \" we get a minhash similarity of \", successCounter/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![features](./images/formula.png)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![features](./images/graph.png)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "def getTokens(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        return data.read().replace('\\n', ' ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = getTokens(files[0])\n",
    "tokens2 = getTokens(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Count Vectors for the documents\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',max_features=500)\n",
    "count_vect.fit(tokens1+tokens2)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "tokens1_count_vector =  count_vect.transform(tokens1).toarray()\n",
    "tokens2_count_vector =  count_vect.transform(tokens2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 177)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 177)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 53631)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to 1D\n",
    "d1 = tokens1_count_vector.reshape(1,-1)\n",
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 53631)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to 1D\n",
    "d2 = tokens2_count_vector.reshape(1,-1)\n",
    "d2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7920792079207919\n"
     ]
    }
   ],
   "source": [
    "print('Cosine Similarity:',cosine_similarity(d1,d2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
